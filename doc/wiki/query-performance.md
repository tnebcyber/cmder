# Best Practices for Optimizing Database Queries

The following tests are based on PostgreSQL. The test data includes:
- 2 million posts
- 5 million authors
- 100,000 categories
- 10 million post-author relationships

You can find my code at [BlogEfExample](https://github.com/fluent-cms/fluent-cms/tree/main/examples/BlogEfExample) and the test scripts along with data population scripts at [performance_tests](https://github.com/fluent-cms/fluent-cms/tree/main/performance_tests).

## Join vs. Merge Result

### Join

The following code was generated by ChatGPT:

```csharp
var posts = await context.Posts
    .Where(p => !p.Deleted && (ts == null || p.PublishedAt.ToLocalTime() < ts))
    .OrderByDescending(p => p.PublishedAt)
    .Take(10)
    .Select(p => new
    {
        p.Id, p.Title, p.PublishedAt, p.Slug, p.CategoryId, p.ThumbnailImage,
        CategoryIdData = context.Categories
            .Where(c => c.Id == p.CategoryId && !c.Deleted)
            .Select(c => new
            {
                c.Id, c.Name, c.ParentCategoryId, c.FeaturedImage, c.ThumbnailImage, c.CreatedAt, c.UpdatedAt, c.Slug
            })
            .FirstOrDefault(),
        Authors = context.PostAuthors
            .Where(ap => ap.PostId == p.Id && !ap.Deleted)
            .Join(context.Authors,
                  ap => ap.AuthorId,
                  a => a.Id,
                  (ap, a) => new
                  {
                      a.Id, a.Name, a.Slug, a.ThumbnailImage, a.FeaturedImage, a.CreatedAt, a.UpdatedAt
                  })
            .ToList()
    })
    .ToListAsync();
```

The generated SQL joins the authors, posts, post-author, and category tables:

```sql
SELECT t.id, t.title, t.published_at, t.slug, t.category_id, t.thumbnail_image, t0."Id", t2."Id", t2."Name", t2."Slug", t2."ThumbnailImage", t2."FeaturedImage", t2."CreatedAt", t2."UpdatedAt", t2.id0, t0."Name", t0."ParentCategoryId", t0."FeaturedImage", t0."ThumbnailImage", t0."CreatedAt", t0."UpdatedAt", t0."Slug", t0.c
FROM (
    SELECT p.id, p.title, p.published_at, p.slug, p.category_id, p.thumbnail_image
    FROM posts AS p
    WHERE NOT (p.deleted)
    ORDER BY p.published_at DESC
    LIMIT @__p_0
) AS t
LEFT JOIN (
    SELECT t1."Id", t1."Name", t1."ParentCategoryId", t1."FeaturedImage", t1."ThumbnailImage", t1."CreatedAt", t1."UpdatedAt", t1."Slug", t1.c
    FROM (
        SELECT c.id AS "Id", c.name AS "Name", c.parent_category_id AS "ParentCategoryId", c.featured_image AS "FeaturedImage", c.thumbnail_image AS "ThumbnailImage", c.created_at AS "CreatedAt", c.updated_at AS "UpdatedAt", c.slug AS "Slug", 1 AS c, ROW_NUMBER() OVER(PARTITION BY c.id ORDER BY c.id) AS row
        FROM categories AS c
        WHERE NOT (c.deleted)
    ) AS t1
    WHERE t1.row <= 1
) AS t0 ON t.category_id = t0."Id"
LEFT JOIN (
    SELECT a0.id AS "Id", a0.name AS "Name", a0.slug AS "Slug", a0.thumbnail_image AS "ThumbnailImage", a0.featured_image AS "FeaturedImage", a0.created_at AS "CreatedAt", a0.updated_at AS "UpdatedAt", a.id AS id0, a.post_id
    FROM author_post AS a
    INNER JOIN authors AS a0 ON a.author_id = a0.id
    WHERE NOT (a.deleted)
) AS t2 ON t.id = t2.post_id
ORDER BY t.published_at DESC, t.id, t0."Id", t2.id0
```

Using k6 test scripts, this approach can handle 180 requests per second:

```shell
http_reqs......................: 38196  180.164067/s
```

### Merge

Here's the optimized code, which first queries the posts table and then uses the post IDs to query authors and categories separately:

```csharp
var posts = await context.Posts
    .Where(p => !p.Deleted && (ts == null || p.PublishedAt.ToLocalTime() < ts))
    .OrderByDescending(p => p.PublishedAt)
    .Take(10)
    .Select(p => new { p.Id, p.Title, p.PublishedAt, p.Slug, p.CategoryId, p.ThumbnailImage })
    .ToListAsync();

var categoryIds = posts.Select(p => p.CategoryId).Distinct().ToList();

var categories = await context.Categories
    .Where(c => categoryIds.Contains(c.Id))
    .Select(c => new { c.Id, c.Name, c.ParentCategoryId, c.FeaturedImage, c.ThumbnailImage, c.CreatedAt, c.UpdatedAt, c.Slug })
    .ToListAsync();

var postIds = posts.Select(p => p.Id).ToList();

var authors = await (from pa in context.PostAuthors
    join a in context.Authors on pa.AuthorId equals a.Id
    where postIds.Contains(pa.PostId) && !pa.Deleted && !a.Deleted
    select new { pa.PostId, Author = new { a.Id, a.Name, a.Slug, a.ThumbnailImage, a.FeaturedImage, a.CreatedAt, a.UpdatedAt } })
    .ToListAsync();

var result = posts.Select(p => new
{
    p.Id, p.Title, p.PublishedAt, p.Slug, p.CategoryId, p.ThumbnailImage,
    CategoryIdData = categories.FirstOrDefault(c => c.Id == p.CategoryId),
    Authors = authors.Where(a => a.PostId == p.Id).Select(a => a.Author).ToList()
}).ToList();
```

Based on my tests, for large datasets, avoiding joins and querying tables individually with a merge strategy might yield better performance:

```shell
http_reqs......................: 465091  2526.042585/s
```

## Offset vs. Cursor

### Offset

For pagination, a common approach is for the frontend to pass an offset to the backend API, which then skips the specified number of records:

```csharp
var posts = await context.Posts
    .Where(p => !p.Deleted)
    .OrderByDescending(p => p.PublishedAt)
    .Skip(offset)
    .Take(10)
    .ToListAsync();
```

This approach works fine for small datasets. However, with a large dataset like mine, where there are 2 million posts, allowing the frontend to pass an offset greater than 1 million causes a significant drop in API performance.

### Cursor

To address this issue, instead of having the frontend pass the offset, we let it pass the cursor of the last record. For example, in this case, the cursor would be the `PublishedAt` timestamp of the last record. The code then changes to:

```csharp
var posts = await context.Posts
    .Where(p => !p.Deleted && (ts == null || p.PublishedAt.ToLocalTime() < ts))
    .OrderByDescending(p => p.PublishedAt)
    .Take(10)
    .ToListAsync();
```

This way, accessing any page of data maintains similar performance to accessing the first page.
